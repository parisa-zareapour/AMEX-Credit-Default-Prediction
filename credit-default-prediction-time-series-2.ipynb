{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Introduction\n\nCredit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.\n\nIn this notebook, I will apply my machine learning skills to predict credit default. Specifically, I will leverage the Kaggle's \"American Express - Default Prediction\" Competition data set to build a machine learning model. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information.\n\n\nThe objective is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\n\nThe dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n\nD_* = Delinquency variables\nS_* = Spend variables\nP_* = Payment variables\nB_* = Balance variables\nR_* = Risk variables\nwith the following features being categorical:\n\n['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\n\nMy purpose in this notebook is to explore and model the time series component of the data.","metadata":{}},{"cell_type":"markdown","source":"### 2. Key Findings","metadata":{}},{"cell_type":"markdown","source":"- In this notebook, I used a simple LSTM network to train time series credit card data from customers.\n\n- I was only able to include a subset of the data in the notebook, since otherwise I was running into memory issues.\n\n- The competition metric I got from this model was 0.635. To improve this result, I will try to solve the memory issue so that I can train the model on the whole dataset.","metadata":{}},{"cell_type":"markdown","source":"### 3. Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom kerastuner.tuners import RandomSearch\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:05:22.029647Z","iopub.execute_input":"2023-10-28T20:05:22.030180Z","iopub.status.idle":"2023-10-28T20:05:38.838433Z","shell.execute_reply.started":"2023-10-28T20:05:22.030153Z","shell.execute_reply":"2023-10-28T20:05:38.837466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Load Dataset","metadata":{}},{"cell_type":"markdown","source":"The dataset of this competition has a considerable size. If I read the original csv files, the data barely fits into memory. That's why I read the data from @munumbutt's AMEX-Feather-Dataset. In this Feather file, the floating point precision has been reduced from 64 bit to 16 bit. And reading a Feather file is faster than reading a csv file because the Feather file format is binary.\n\nThere are 5.5 million rows of data in this dataset. I ran out of memory in the model training stage. So I am only reading the first 900000 rows of data.\n\nI will focus on the numerical features. There are 11 categorical features, which I will drop.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_feather(\"../input/amexfeather/train_data.ftr\").iloc[:900000,:]\ncategorical_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\ntrain_data = train_data.drop(columns = categorical_features)\ntrain_data['Date Time'] = pd.to_datetime(train_data['S_2'], format='%d.%m.%Y %H:%M:%S')\ntrain_data = train_data.drop(columns='S_2')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:05:38.840463Z","iopub.execute_input":"2023-10-28T20:05:38.841421Z","iopub.status.idle":"2023-10-28T20:05:58.779007Z","shell.execute_reply.started":"2023-10-28T20:05:38.841368Z","shell.execute_reply":"2023-10-28T20:05:58.778233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Preprocess the Data","metadata":{}},{"cell_type":"markdown","source":"80% of the customers have 13 statements. The remaining 20% of the customers have 1 to 12 statements. For purpose of time-series modelling, I will only include customers that have 13 statements.","metadata":{}},{"cell_type":"code","source":"train_data.customer_ID.value_counts().value_counts().sort_index(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data[train_data['customer_ID'].map(train_data['customer_ID'].value_counts()) == 13]","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:09:19.961201Z","iopub.execute_input":"2023-10-28T20:09:19.961555Z","iopub.status.idle":"2023-10-28T20:09:21.338844Z","shell.execute_reply.started":"2023-10-28T20:09:19.961524Z","shell.execute_reply":"2023-10-28T20:09:21.337817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a significant number of missing data. It is not reasonable to drop all columns or rows that have a missing value.\n\nNeural networks can not deal with missing values. So I need to impute values for NNs. I will sort the train data first by costumer_ID and then by date (S_2). I will then interpolate linearly the missing values,","metadata":{}},{"cell_type":"code","source":"null= pd.DataFrame(train_data.isnull().sum(),columns=['number_of_nulls'])\nnull['percentage_of_null'] = round(((null['number_of_nulls']/len(train_data))*100) , 2)\nnull = null[null['number_of_nulls']>0]\nnull= null.sort_values(by='percentage_of_null',ascending=False)\nnull.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.sort_values(['customer_ID', 'S_2'])\ntrain_data.interpolate(method='linear', inplace=True, limit_direction='both')\n#train_data = train_data.fillna(0)\ntrain_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:10:40.376928Z","iopub.execute_input":"2023-10-28T20:10:40.377551Z","iopub.status.idle":"2023-10-28T20:10:40.825164Z","shell.execute_reply.started":"2023-10-28T20:10:40.377519Z","shell.execute_reply":"2023-10-28T20:10:40.824288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:20:45.930124Z","iopub.execute_input":"2023-10-28T20:20:45.930452Z","iopub.status.idle":"2023-10-28T20:21:04.938988Z","shell.execute_reply.started":"2023-10-28T20:20:45.930427Z","shell.execute_reply":"2023-10-28T20:21:04.938094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize a few features as a function of time and see how they evolve.\n\nWe have 177 features. I will select top 10 features to plot. Top features are selected based on the LightGBM feature importance I did in another notebook.","metadata":{}},{"cell_type":"code","source":"# Most important features based on LGBM feature importance\ntop_features = [\"P_2\",\"D_39\", \"S_3\",\"B_4\", \"D_43\",\"D_42\",\"B_3\",\"B_5\",\"D_46\",\"D_49\"]\ntitles = top_features\n\ncolors = [\"blue\",  \"orange\",  \"green\",  \"red\",  \"purple\",  \"brown\",  \"pink\",  \"gray\",  \"olive\",  \"cyan\"]\n\ndef show_raw_visualization(data):\n    time_data = data['S_2']\n    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n    for i in range(len(top_features)):\n        key = top_features[i]\n        c = colors[i % (len(colors))]\n        t_data = data[key]\n        t_data.index = time_data\n        t_data.head()\n        ax = t_data.plot(\n            ax=axes[i // 2, i % 2],\n            color=c,\n            title=\"{}\".format(titles[i]),\n            rot=25)\n        ax.legend([titles[i]])\n    plt.tight_layout()\n\nshow_raw_visualization(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:22:39.160841Z","iopub.execute_input":"2023-10-28T20:22:39.161220Z","iopub.status.idle":"2023-10-28T20:23:52.188838Z","shell.execute_reply.started":"2023-10-28T20:22:39.161189Z","shell.execute_reply":"2023-10-28T20:23:52.187807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standardize the Data","metadata":{}},{"cell_type":"markdown","source":"Each feature has a different range. This is not ideal for a neural network. In general It is better to normalize the input values. Data normalization is a crucial preprocessing step when working with neural networks to ensure stable and efficient training, faster convergence, and better generalization on the data\n\nI will do normalization to confine feature values to a range of [0, 1] before training a neural network. I do this by subtracting the mean and dividing by the standard deviation of each feature.","metadata":{}},{"cell_type":"code","source":"def normalize(data, train_split):\n    data_mean = data[:train_split].mean(axis=0)\n    data_std = data[:train_split].std(axis=0)\n    return (data - data_mean) / data_std","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:33:18.954688Z","iopub.execute_input":"2023-10-28T20:33:18.955054Z","iopub.status.idle":"2023-10-28T20:33:18.960139Z","shell.execute_reply.started":"2023-10-28T20:33:18.955012Z","shell.execute_reply":"2023-10-28T20:33:18.959111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"75% of the data will be used to train the model.\n\n25% of the data will be used for validation.","metadata":{}},{"cell_type":"code","source":"train_data.index = train_data['Date Time']\nsplit_fraction = 0.75\ntrain_split = int(split_fraction * int(train_data.shape[0]))\ntrain_split = train_split + 13 - train_split%13\ntrain_data.iloc[:,1:-2] = normalize(train_data.iloc[:,1:-2].values, train_split)\n\ntra_data = train_data.iloc[0 : train_split - 1]\nval_data = train_data.iloc[train_split:]\ndel train_data","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:33:24.436582Z","iopub.execute_input":"2023-10-28T20:33:24.437293Z","iopub.status.idle":"2023-10-28T20:33:33.733326Z","shell.execute_reply.started":"2023-10-28T20:33:24.437261Z","shell.execute_reply":"2023-10-28T20:33:33.732527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = tra_data.iloc[:,1:-1].values\ny_train = tra_data['target'].values\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:33:34.319751Z","iopub.execute_input":"2023-10-28T20:33:34.320053Z","iopub.status.idle":"2023-10-28T20:33:34.750600Z","shell.execute_reply.started":"2023-10-28T20:33:34.320009Z","shell.execute_reply":"2023-10-28T20:33:34.749495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val = val_data.iloc[:,1:-1].values\ny_val = val_data['target'].values\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:33:35.025743Z","iopub.execute_input":"2023-10-28T20:33:35.026090Z","iopub.status.idle":"2023-10-28T20:33:35.178163Z","shell.execute_reply.started":"2023-10-28T20:33:35.026061Z","shell.execute_reply":"2023-10-28T20:33:35.177186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The timeseries_dataset_from_array function takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as length of the sequences/windows, spacing between two sequence/windows, etc., to produce batches of sub-timeseries inputs and targets sampled from the main timeseries.\n\nThe following is the train and the validation datasets.","metadata":{}},{"cell_type":"code","source":"dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n    x_train,\n    y_train,\n    sequence_length=13,\n    sampling_rate=1,\n    batch_size=512,\n)\n\n\ndataset_val = keras.preprocessing.timeseries_dataset_from_array(\n    x_val,\n    y_val,\n    sequence_length=13,\n    sampling_rate=1,\n    batch_size=512,\n)\n\n\nfor batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:33:42.800101Z","iopub.execute_input":"2023-10-28T20:33:42.800397Z","iopub.status.idle":"2023-10-28T20:33:43.499079Z","shell.execute_reply.started":"2023-10-28T20:33:42.800370Z","shell.execute_reply":"2023-10-28T20:33:43.498070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Model Training","metadata":{}},{"cell_type":"markdown","source":"I will define a simple Long Short-Term Memory (LSTM) network using the Keras API.\n\nLSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture that is well-suited for learning from sequences of data. It is designed to overcome the limitations of traditional RNNs in capturing long-term dependencies in sequential data.\n\nIn sequential data, such as time series data, the order of the elements carries crucial information. However, traditional RNNs face challenges in learning and retaining information over long sequences due to the vanishing gradient problem, where the gradients of the loss function tend to become very small, leading to slow or no learning.\n\nI will use binary_crossentropy as my loss metric. Binary Cross Entropy, also known as Log Loss, is a loss function used in binary classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n\nI will use sigmoid as much activation output layer. The sigmoid activation function is appropriate for use as the output layer of a binary classification model, where it squashes the final output to represent the probability of belonging to a certain class.\n\nLet's see how this simple network will perform for this dataset.","metadata":{}},{"cell_type":"code","source":"learning_rate=0.0001\n\ninputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\nlstm_out = keras.layers.LSTM(32)(inputs)\noutputs = keras.layers.Dense(1, activation=\"sigmoid\")(lstm_out)\n\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T21:31:26.981406Z","iopub.execute_input":"2023-10-28T21:31:26.982273Z","iopub.status.idle":"2023-10-28T21:31:27.262991Z","shell.execute_reply.started":"2023-10-28T21:31:26.982230Z","shell.execute_reply":"2023-10-28T21:31:27.262078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use the ModelCheckpoint callback to regularly save checkpoints.\n\nI will use EarlyStopping callback to interrupt training when the validation loss is not longer improving.","metadata":{}},{"cell_type":"code","source":"path_checkpoint = \"model_checkpoint.h5\"\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.00001, patience=5)\n\nmodelckpt_callback = keras.callbacks.ModelCheckpoint(\n    monitor=\"val_loss\",\n    filepath=path_checkpoint,\n    verbose=1,\n    save_weights_only=True,\n    save_best_only=True,\n)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=50,\n    validation_data=dataset_val,\n    callbacks=[es_callback, modelckpt_callback],\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T21:31:27.536060Z","iopub.execute_input":"2023-10-28T21:31:27.536412Z","iopub.status.idle":"2023-10-28T22:14:49.378915Z","shell.execute_reply.started":"2023-10-28T21:31:27.536385Z","shell.execute_reply":"2023-10-28T22:14:49.377922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model stopped after 44 epochs, because validation loss was no longer improving.","metadata":{}},{"cell_type":"markdown","source":"### Visualize Loss\n\nI will visualize the loss with the function below. At one point, the validation loss stops decreasing, even though the training loss keeps going down.\n\nThe training was stopped when validation loss was no longer decreasing (Early Stpping).","metadata":{}},{"cell_type":"code","source":"def visualize_loss(history, title):\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(len(loss))\n    plt.figure()\n    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_loss(history, \"Training and Validation Loss\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T22:19:33.703043Z","iopub.execute_input":"2023-10-28T22:19:33.703949Z","iopub.status.idle":"2023-10-28T22:19:33.999296Z","shell.execute_reply.started":"2023-10-28T22:19:33.703914Z","shell.execute_reply":"2023-10-28T22:19:33.998464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction\n","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(dataset_val)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T22:19:36.393489Z","iopub.execute_input":"2023-10-28T22:19:36.394325Z","iopub.status.idle":"2023-10-28T22:19:51.794441Z","shell.execute_reply.started":"2023-10-28T22:19:36.394290Z","shell.execute_reply":"2023-10-28T22:19:51.793648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Competition Metric\n\nThe evaluation metric, ð‘€, for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, ðº, and default rate captured at 4%, ð·.\n\nð‘€=0.5â‹…(ðº+ð·)\n\nThe default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic.\n\nFor both of the sub-metrics ðº and ð·, the negative labels are given a weight of 20 to adjust for downsampling.\n\nThis metric has a maximum value of 1.0.\n\nThis is the code for calculating this metric, provided by the competition.\n","metadata":{}},{"cell_type":"code","source":"def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n\n    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        four_pct_cutoff = int(0.04 * df['weight'].sum())\n        df['weight_cumsum'] = df['weight'].cumsum()\n        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n        \n    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n        total_pos = (df['target'] * df['weight']).sum()\n        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n        df['lorentz'] = df['cum_pos_found'] / total_pos\n        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n        return df['gini'].sum()\n\n    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        y_true_pred = y_true.rename('prediction')\n        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n\n    g = normalized_weighted_gini(y_true, y_pred)\n    d = top_four_percent_captured(y_true, y_pred)\n\n    return 0.5 * (g + d)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T22:20:13.188443Z","iopub.execute_input":"2023-10-28T22:20:13.188816Z","iopub.status.idle":"2023-10-28T22:20:13.201299Z","shell.execute_reply.started":"2023-10-28T22:20:13.188784Z","shell.execute_reply":"2023-10-28T22:20:13.200320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pd.Series(y_pred.reshape(y_pred.shape[0])).rename('prediction', inplace=True)\ny_true = pd.Series(y_val).reset_index(drop=True).rename('target', inplace=True)\ny_pred = y_pred.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T22:20:13.511420Z","iopub.execute_input":"2023-10-28T22:20:13.511730Z","iopub.status.idle":"2023-10-28T22:20:13.518248Z","shell.execute_reply.started":"2023-10-28T22:20:13.511704Z","shell.execute_reply":"2023-10-28T22:20:13.517472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amex_metric(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T22:20:17.600153Z","iopub.execute_input":"2023-10-28T22:20:17.600514Z","iopub.status.idle":"2023-10-28T22:20:18.082399Z","shell.execute_reply.started":"2023-10-28T22:20:17.600484Z","shell.execute_reply":"2023-10-28T22:20:18.081473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:\n\n- The competition metric I got from my neural network was 0.635. This is not bad and it is better than a random chance.\n\n- There are ways to improve these results. My next step is to try tf.data API to solve the memory issues. That will allow me to include all the training data for my model training.\n\n- Another future step for this project would be to use keras tuner to tune hyperparameters, number of layers, learning rate, etc.","metadata":{}}]}